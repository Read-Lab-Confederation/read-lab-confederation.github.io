<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>EMERGENT | Using AWS Batch to process 67,000 genomes with Bactopia</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=generator content="Hugo 0.79.1"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link href=/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css rel=stylesheet><link rel=stylesheet href=/css/custom.css><meta property="og:title" content="Using AWS Batch to process 67,000 genomes with Bactopia"><meta property="og:description" content="In 5 days, I processed 67,000 Staphylococcus aureus genomes using Bactopia and Amazon Web Services (AWS). What would have taken 6 months using our local resources, was done in less than a week&rsquo;s time. (magical)
More importantly, in October 2019 I received an AWS Cloud Credits for Research award that expired on October 31, 2020. I had plenty of time, plenty. Well&mldr; yeah&mldr; out of nowhere October 2020 shows up and is all like &ldquo;OH HIIII!"><meta property="og:type" content="article"><meta property="og:url" content="https://read-lab-confederation.github.io/blog/posts/bactopia-aws-and-67000-genomes/"><meta property="article:published_time" content="2020-12-07T00:00:00+00:00"><meta property="article:modified_time" content="2020-12-07T00:00:00+00:00"><meta itemprop=name content="Using AWS Batch to process 67,000 genomes with Bactopia"><meta itemprop=description content="In 5 days, I processed 67,000 Staphylococcus aureus genomes using Bactopia and Amazon Web Services (AWS). What would have taken 6 months using our local resources, was done in less than a week&rsquo;s time. (magical)
More importantly, in October 2019 I received an AWS Cloud Credits for Research award that expired on October 31, 2020. I had plenty of time, plenty. Well&mldr; yeah&mldr; out of nowhere October 2020 shows up and is all like &ldquo;OH HIIII!"><meta itemprop=datePublished content="2020-12-07T00:00:00+00:00"><meta itemprop=dateModified content="2020-12-07T00:00:00+00:00"><meta itemprop=wordCount content="2513"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Using AWS Batch to process 67,000 genomes with Bactopia"><meta name=twitter:description content="In 5 days, I processed 67,000 Staphylococcus aureus genomes using Bactopia and Amazon Web Services (AWS). What would have taken 6 months using our local resources, was done in less than a week&rsquo;s time. (magical)
More importantly, in October 2019 I received an AWS Cloud Credits for Research award that expired on October 31, 2020. I had plenty of time, plenty. Well&mldr; yeah&mldr; out of nowhere October 2020 shows up and is all like &ldquo;OH HIIII!"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-148959477-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link href=/css/share-button.css rel=stylesheet></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://read-lab-confederation.github.io/images/posts/bactopia-aws-and-67000-genomes/final-nf-tower.png)><div class="pb3-m pb6-l bg-black-60"><nav class="pv3 ph3 ph4-ns tc tl-l" role=navigation><div class="flex-l justify-between items-center center"><a href=https://read-lab-confederation.github.io class="f3 fw2 hover-white no-underline white-90 dib">EMERGENT</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f7 f4-ns fw4 dib pr1 pr3-ns"><a class="hover-white no-underline white-90" href=/about/ title="About page">About</a></li><li class="list f7 f4-ns fw4 dib pr1 pr3-ns"><a class="hover-white no-underline white-90" href=/blog/ title="Blog page">Blog</a></li><li class="list f7 f4-ns fw4 dib pr1 pr3-ns"><a class="hover-white no-underline white-90" href=/group/ title="Group page">Group</a></li><li class="list f7 f4-ns fw4 dib pr1 pr3-ns"><a class="hover-white no-underline white-90" href=/research/ title="Research page">Research</a></li><li class="list f5 f4-ns fw4 dib pr1 pr3-ns"><a href=https://twitter.com/tdread_emory target=_blank class="link-transition twitter link dib z-999 pt0-l mr1" title="Twitter link" rel=noopener aria-label="follow on Twitter——Opens in a new window"><svg height="24" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" width="8" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></span></a></li></ul></div></div></nav><div class="tc-l pv6-l pv4 ph3 ph4-ns"><h1 class="dib-ns dn f1-ns fw2 white-90 mb0 lh-title">Using AWS Batch to process 67,000 genomes with Bactopia</h1></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><p class="f6 b helvetica tracked">POSTS</p><h1 class="f3-ns f4 athelas mb1">Using AWS Batch to process 67,000 genomes with Bactopia</h1><p class="f4-ns f1 athelas mb1"><em>How we spent $12K of Jeff Bezos' cash</em></p></br><time class="f6 dib tracked" datetime=2020-12-07T00:00:00Z>December 7, 2020</time><div class=author-info><p class=author-name>Robert Petit</p></div></header><section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-80-l"><p>In 5 days, I processed 67,000 <em>Staphylococcus aureus</em> genomes using <a href=https://bactopia.github.io/>Bactopia</a> and Amazon Web Services (AWS). What would have taken 6 months using our local resources, was done in less than a week&rsquo;s time. (<em>magical</em>)</p><p>More importantly, in October 2019 I received an AWS Cloud Credits for Research award that expired on October 31, 2020. I had plenty of time, plenty. Well&mldr; yeah&mldr; out of nowhere October 2020 shows up and is all like &ldquo;<em>OH HIIII!</em>&rdquo;. So, the genomes had to be processed ASAP, otherwise the credits went <em>POOF</em>.</p><p>This post outlines my recent experience.</p><p>I&rsquo;ll show that <a href=https://doi.org/10.1128/msystems.00190-20>Bactopia</a> can use AWS to process thousands of genomes reliably and cost effectively. I&rsquo;ll give a run down of how I went from testing a few jobs locally to submitting 67,000 jobs to AWS Batch. This involved getting down into some of the details of how AWS works with Nextflow pipelines and containers along with a lot of optimization. I will also give a breakdown of the costs associated with using AWS.</p><p>If you are comfortable processing bacterial genomes and creating your own workflows, but are unsure of how the cloud might work for you, I think this post presents a good <em>use-case</em> while also highlighting the costs associated in doing so.</p><p>So, let&rsquo;s get started!</p><h2 id=getting-started>Getting Started</h2><h3 id=acquiring-s-aureus-genome-list>Acquiring <em>S. aureus</em> Genome List</h3><p>A great thing about Bactopia is you can feed it Sequence Read Archive Experiment (e.g. SRX0000001) as an input. Bactopia will take that accession, download the FASTQs and start processing them. Even better you can use <code>bactopia search</code> to <a href=https://bactopia.github.io/usage-basic/#generating-accession-list>generate the list of accessions</a>. So in order to get a list of <em>S. aureus</em> publicly available genomes, I used the following command:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>bactopia search <span style=color:#e6db74>&#34;Staphylococcus aureus&#34;</span> --prefix saureus
</code></pre></div><p>From this command 3 files are created, one of those was <code>saureus-accessions.txt</code> which contained the full list of SRA Experiment accessions that were labeled as <em>S. aureus</em>. This accessions file could then be used by Bactopia, like this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>bactopia --accessions saureus-accessions.txt
</code></pre></div><h3 id=getting-setup-for-aws-batch>Getting Setup for AWS Batch</h3><p>Bactopia is built using Nextflow, which natively supports many different <a href=https://www.nextflow.io/docs/latest/executor.html>job executors</a>. With a simple parameter change (e.g. <code>-profile slurm</code>) Nextflow will handle submitting jobs to the specified executor. One of the supported executors is <a href=https://aws.amazon.com/batch/>AWS Batch</a> which allows you to submit jobs directly to AWS without having to manage a cluster.</p><p>In order to get started using AWS Batch with Nextflow, I followed Dr. Anthony Underwood&rsquo;s <em><a href=https://antunderwood.gitlab.io/bioinformant-blog/posts/running_nextflow_on_aws_batch/>Running Nextflow on AWS Batch</a></em> and Tobias Neumann&rsquo;s <em><a href=https://t-neumann.github.io/pipelines/AWS-pipeline/>Pipelines on AWS</a></em>. After going through these posts, there really isn&rsquo;t much I can add to the setup process that they don&rsquo;t already thoroughly cover.</p><h2 id=the-testing-phase>The Testing Phase</h2><h3 id=phase-1-does-bactopia-work-on-aws-batch-out-of-the-box>Phase 1: Does Bactopia work on AWS Batch out of the box?</h3><p>Short answer: <em>nope!</em></p><p>While Nextflow works with AWS Batch, Bactopia needed some adjustments to work with Batch.</p><p>My testing of Bactopia had mostly been done on a Linux machine using Conda or a SLURM cluster with Singularity images. I had never actually tested Bactopia on AWS Batch but I already had all the Docker containers available. So, I gave it a go and quickly learned a few things.</p><ol><li>AWS Batch did not like the extensive use of symbolic links for output files</li><li>Sometimes a process would start before all inputs were staged</li><li>Pulling in Docker containers was a time sink</li></ol><p>#1 was easy enough to solve, in cases where I was passing a symbolic link I added a <a href=https://github.com/bactopia/bactopia/blob/master/templates/estimate_genome_size.sh#L87-L105>check to choose to copy or create a link</a>.</p><p>#2 was a <a href=https://github.com/nextflow-io/nextflow/issues/1107>known issue</a> that would occur when an EC2 instance made too many S3 requests. This required me to write a <a href=https://github.com/bactopia/bactopia/blob/master/bin/check_staging.py>script</a> that would verify all input files were available and if not, exit, then retry the process.</p><p>#3 was an issue because Bactopia makes use of 12 different Docker containers. For each container it was taking 1-4 minutes to pull the container from DockerHub to AWS. Bactopia has 29 different processes, so that is 29 container pulls or 29-116 minutes per genome of just pulling containers. <em>Not cool!</em></p><p>I solved the first two issues, and set the Docker issue to the side. I was able to process genomes (yay!), but only slowly. For a random set of 100 genomes, it was taking 3-4 hours to complete and failed jobs still occurred because eventually the staged file check would reach maximum retries.</p><p>This led me back to the drawing board.</p><h3 id=phase-2-could-i-use-slurm-on-aws>Phase 2: Could I use SLURM on AWS?</h3><p>AWS Batch was nice, but the speed was not the best and the file staging was causing too many failures. Given I knew Bactopia worked on SLURM, a quick Google search for <em><a href="https://www.google.com/search?q=SLURM+on+AWS&oq=SLURM+on+AWS">&ldquo;SLURM on AWS&rdquo;</a></em> brought me to <a href=https://aws.amazon.com/hpc/parallelcluster/>AWS Parallel Cluster</a>, a tool that builds a SLURM cluster on AWS for you. It also automated Network File Shares (NFS) across nodes using AWS Elastic Block Store (EBS) volumes and allowed me to bypass the whole S3 unstaged file situation (a big win!).</p><p>After getting Parallel Cluster going, I set up all the datasets, imported Singularity images, and was off testing again. I ran a single genome through and it worked great! It was quick and smooth. I then launched another job, this time with 100 genomes. It failed due to:</p><p><em>no more spot-instances available in your availability zone</em></p><p>Ok? That&rsquo;s weird? My AWS accounts limits are pretty high, so it that shouldn&rsquo;t be an issue. My first thought was to change availability zones. Maybe some one else is trying to spend all their credits before Halloween?</p><p>In order to change availability zones, this required shutting down the cluster and creating a new one. Even better, Parallel Cluster does not allow you to choose a zone it just <em>&ldquo;randomly&rdquo;</em> picks one. So guess what?</p><p>You know it! I got the same availability zone on the next cluster! Haha I played the <em>random</em> game a few times, finally got a different zone, and restarted the setup process again.</p><p>All set up, new zone, let&rsquo;s go! &mldr;</p><p><em>no more spot-instances available</em></p><p>Well, that&rsquo;s annoying, let&rsquo;s try a different instance type.</p><p><em>no more spot-instances available</em></p><p>Hmmm, well I guess another zone?</p><p>I eventually edited the Parallel Cluster launch code to force an availability zone, and tried all of the us-east-1 zones. In every zone, no matter the instance type I tried, I eventually got the <em>no more spot-instances available</em> error. Again, and unfortunately, this error was not due to limits on my AWS account. Otherwise it would have been an easy fix (<em>&ldquo;Hey AWS, please, can I have some more?"</em>).</p><p>I was really rooting for Parallel Cluster, but at the end of the day it just didn&rsquo;t work out. So it was back to AWS Batch.</p><h3 id=phase-3-back-to-aws-batch-how-can-i-pull-docker-containers-faster>Phase 3 (Back to AWS Batch): How can I pull Docker containers faster?</h3><p>Now that I was back on AWS Batch, I decided it was time to solve the whole <em>&ldquo;pulling Docker containers taking forever&rdquo;</em> issue. Because waiting 30-120 minutes for containers, per-genome, is an obscene amount of wasted time (and money!).</p><p>I had a few options in mind, put the containers on the AWS Elastic Container Registry (ECR) or create an Amazon Machine Image (AMI) to <em>&ldquo;pre-pull&rdquo;</em> them. I thought about using ECR, but it required authentication to pull images (unless I missed something) so I went the custom AMI route.</p><p>This process was a lot of trial and error. To make matters a little more tedious, AMIs cannot be changed on existing AWS Batch compute environments. For every AMI I created, I had create a new compute environment for AWS Batch. I&rsquo;m sure there&rsquo;s some automated method, but not for this amateur!</p><p>Well, I tried pre-building Conda environments:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># Problematic frame:</span>
<span style=color:#75715e># C  [libc.so.6+0x14b35f][thread 3959 also had an error]</span>
  __memmove_avx_unaligned_erms+0x4f
</code></pre></div><p>Never mind, I then tried Singularity in Docker:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>env: <span style=color:#e6db74>&#39;singularity&#39;</span>: No such file or directory <span style=color:#f92672>(</span>Fixed: added to container<span style=color:#f92672>)</span>
ERROR  : Failed to unshare root file system: Operation not permitted <span style=color:#f92672>(</span>tried --priveledged<span style=color:#f92672>)</span>
</code></pre></div><p>I tried Docker in Docker:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>.command.run: line 259: docker: command not found <span style=color:#f92672>(</span>Fixed: installed docker on container<span style=color:#f92672>)</span>
docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>        Is the docker daemon running?. <span style=color:#f92672>(</span>Fixed: Mounted /var/run/docker<span style=color:#f92672>)</span>
/bin/bash: .command.run: No such file or directory
</code></pre></div><p>Don&rsquo;t judge! It was late and I was trying to make it work! There were other attempts, but those were lost to the</p><p>Eventually, I circled back to the Conda error. It should have worked, it was the simplest and most straight-forward method. None of that containers within containers bit. After some digging&mldr;</p><p><em>sigh</em></p><p>It was a simple fix, the issue was related to building the Conda environments on an Intel-based instance and then trying to run them on an AMD-based instance. Some of the tools took advantage of instructions available to the Intel instances and not the AMD ones.</p><p>My solution? Don&rsquo;t mix Intel and AMD, and since there were more Intel instance options I only used Intel-based instances.</p><p>After <a href=https://gist.github.com/rpetit3/01f27f52273104bbf609ed5acc9b63a9>creating the custom AMI</a> and an <a href=https://github.com/bactopia/bactopia/blob/master/.awsbatch.Dockerfile>AWS Batch Docker container</a>, it took less than 30 seconds to pull in the Docker container. Problem #3 solved! Back to testing.</p><p>Things were much faster! But soon enough, the unstaged file issue would eventually pop up. It really became an issue when I was trying to run tests with 1,000 genomes. I tried reducing the maximum number of simultaneous transfers, and increasing the maximum number of retries per transfer. But eventually, the issue would pop up.</p><p>Back to problem #2.</p><h3 id=phase-4-how-can-i-reduce-s3-transfers-and-save-time-and-money>Phase 4: How can I reduce S3 transfers and save time (<em>and money</em>)?</h3><p>Bactopia has lots of files being moved between processes, so for AWS Batch there are always numerous files being uploaded and downloaded. At this point, I fell back on my experience processing <a href=https://dx.doi.org/10.7717%2Fpeerj.5261>40k genomes for Staphopia</a>.</p><p>For Staphopia, I used a Python wrapper around the Nextflow pipeline to run everything on a single instance and create a final tarball of the results.</p><p>Except this time for Bactopia, I created a <a href=https://gist.github.com/rpetit3/fe1f5428be135852ec90bfb63aa32c93>Nextflow workflow around Bactopia</a>, a <em>Nexflowception</em> if you will!</p><p>This was the solution! So, much smoother than that whole container within container debacle.</p><p>With this wrapper, a genome was processed from start to finish on a single instance and only the final tarball of the results was uploaded to S3. This meant all the intermediate files were never uploaded to S3 significantly reducing the chance of the <em>unstaged files</em> error.</p><p>A consequence of this approach was I couldn&rsquo;t take advantage of Nextflow&rsquo;s queue manager. I had to limit each genome to 4 cores and 16GB of memory. Otherwise, AWS Batch would end up setting aside a lot of resources that weren&rsquo;t being used efficiently.</p><p>During all this testing, I probably tested ~5,000 genomes (only ~2,000 unique) and got a really good idea of the cost per genome and expected runtime/compute requirements. It was time to start processing all the genomes.</p><p>Every one of them.</p><h2 id=the-launch-phase>The Launch Phase</h2><p>Honestly there isn&rsquo;t much to say here, a lot happened but at the same time not much happened. I had planned to submit genomes in batches of 5,000 or so, but decided:</p><p><img src=/images/posts/bactopia-aws-and-67000-genomes/submit-all.png alt="Final Tower Stats"></p><p>I went for it, and submitted all of the remaining 65,000 genomes at once. I also used <a href=https://tower.nf/>Nextflow Tower</a> to track the progress, which made it more fun and easy to track.</p><p>For the next 5 days, genomes were processed and it all went as smooth as I could hope for.</p><p><em>And, then it was done.</em></p><p>Since I used Nextflow Tower I was able to get some pretty nice stats associated with jobs.</p><p>Basically, I used a crap ton of resources over those 5 days!</p><p><img src=/images/posts/bactopia-aws-and-67000-genomes/final-nf-tower.png alt="Final Tower Stats"></p><p>Nearly all jobs took around 55 minutes to complete using only 4-cores and 16GB of memory. This is actually (at least I think so!) pretty cool, because it shows that you don&rsquo;t necessarily need a beefy server to run Bactopia. In most cases, a basic desktop is enough, although it might take longer.</p><p><img src=/images/posts/bactopia-aws-and-67000-genomes/job-duration.png alt="Job Duration"></p><h2 id=how-much-did-it-cost>How much did it cost?</h2><p>Total: <em>$12,408 (USD)</em></p><p>This cost included testing and processing all 67,000 genomes. When I add up the testing and final processing I probably processed close to 70,000 genomes, which comes out to ~$0.18 (USD) to process a <em>S. aureus</em> genome using Bactopia and AWS Batch. Here is the breakdown of costs.</p><table><thead><tr><th>AWS Charge</th><th>Usage Estimate</th><th>Cost USD (%)</th><th style=text-align:center>Cost Per Genome</th></tr></thead><tbody><tr><td>Elastic Compute Cloud (EC2)</td><td>258,670 CPU hours</td><td>$7,552 (61%)</td><td style=text-align:center>$0.11</td></tr><tr><td>Data Transfer</td><td>42 TB</td><td>$3,566 (29%)</td><td style=text-align:center>$0.05</td></tr><tr><td>Simple Storage Service (S3)</td><td>84 TB</td><td>$1,286 (10%)</td><td style=text-align:center>$0.02</td></tr><tr><td>Cloud Watch</td><td>9 GB</td><td>$4 (0%)</td><td style=text-align:center>$0.00</td></tr></tbody></table><p>The bulk of the cost was compute costs, which was about $0.11 per genome. The remaining costs, $0.07, were associated with the storage and transfer of data. I do think there is room to reduce these costs per genome.</p><p>In 2017, I was able to process <em>S. aureus</em> genomes for $0.05 per genome using Staphopia on the <a href=https://www.cancergenomicscloud.org/>Cancer Genomics Cloud (CGC)</a>. Staphopia and Bactopia have very similar runtimes and compute requirements (~50 minutes, 4-cores, 16GB memory). I think optimizing EC2 spot-instance types, such as using only smaller ones, could potentially cut the compute costs in half.</p><p>You might have noticed, my S3 storage (84TB) is double my data transfer (42 TB). This is because I told Nextflow to output the results. If I had told Nextflow to link the result instead of copying, I could have easily cut the S3 storage in half, saving ~$640.</p><p>Unfortunately, there is very little room to reduce data transfer costs. In my case I was already only downloading a gzipped (<code>--best</code>) tarball of all the results per sample. I could have reduced the tarball size by removing some of the result files (FASTQs or BAMs). But, to regenerate these files again would have taken a considerable amount of time. So, for me at least, it made more sense to not delete any of the results and keep them all in the final tarball.</p><p>In the end, with a few tweaks, I could have potentially reduced the costs from $0.18/genome down to $0.11/genome, with roughly half of the $0.11 being just data transfer costs.</p><p>Although, the cost savings would have come with a hit on total processing time. It&rsquo;s likely by using only small instance types, the total processing time would have taken multiple weeks. Given I was on a deadline to spend those credits, I think completing everything in less than a week was a fair trade off.</p><p>Fortunately, all the costs, except $60, were covered by an <a href=https://aws.amazon.com/research-credits/>AWS Cloud Credits for Research</a> award.</p><h2 id=so-whats-next>So, what&rsquo;s next?</h2><p>The plan for these data is to become the basis for Staphopia V2. At the time of writing this, I am still in the midst of transferring data between servers (gotta have those backups!), so we haven&rsquo;t actually gotten the chance to start digging into the data. So, stand by! It&rsquo;s coming soon!</p><h2 id=conclusions-tldr>Conclusions (TL;DR)</h2><ul><li>Nextflow works with AWS Batch, but Bactopia needed adjustments</li><li><a href=https://aws.amazon.com/hpc/parallelcluster/>AWS Parallel Cluster</a> did not work for this case</li><li>Trial and error: Conda, Docker, Singularity, and S3 file staging issues</li><li><em>Nextflow-ception</em> (Bactopia in a Nextflow wrapper) worked best</li><li>Bactopia can be processed small machines (4-core, 16GB memory)</li><li>Bactopia cost ~$0.18 per genome using AWS</li></ul><ul class=pa0></ul><div class=mt6></div></section><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://read-lab-confederation.github.io>&copy; 2024 EMERGENT</a><div><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?text=Check out EMERGENT, a bacterial genomics and metagenomics blog from @tdread_emory's group!&url=https://read-lab-confederation.github.io" target=_blank rel=noopener aria-label="Share on Twitter"><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--large"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5.0-4.55 2.04-4.55 4.54.0.36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3.0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35.0 12.92-6.92 12.92-12.93.0-.2.0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/></svg></div>Share on Twitter</div></a></div></div></footer><script src=/dist/js/app.3fc0f988d21662902933.js></script><script src=/js/custom.js></script></body></html>